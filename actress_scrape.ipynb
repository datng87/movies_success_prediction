{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import requests\n",
    "import pymongo\n",
    "import pandas as pd\n",
    "from random import randint\n",
    "from time import sleep\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "actress_points = []\n",
    "actress_names = []\n",
    "actress_ids = []\n",
    "for j in range(1, 9):\n",
    "    # initiate a new chrome session\n",
    "    executable_path = {'executable_path': ChromeDriverManager().install()}\n",
    "    browser = Browser('chrome', **executable_path, headless=False)\n",
    "    url = f\"https://www.imdb.com/list/ls022928836/?page={j}\"\n",
    "    browser.visit(url)\n",
    "    html = browser.html\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    sleep(randint(1, 3))\n",
    "    browser.quit()\n",
    "    point_soups = soup.find_all('div', class_=\"list-description\")\n",
    "    actress_soups = soup.find_all('a', href=re.compile(\"/name/nm\"))\n",
    "    # scrape points\n",
    "    for i in range(1, len(point_soups)):\n",
    "        try:\n",
    "            point = int(point_soups[i].find('p').text.split()[0])\n",
    "            actress_points.append(point)\n",
    "        except IndexError:\n",
    "            break\n",
    "    #scrape actresses names and ids\n",
    "    for i in range(0, len(actress_soups)):\n",
    "        x = actress_soups[i].find('img')\n",
    "        #check if the attribute has image attribute\n",
    "        if x is None:  #not the actress\n",
    "            continue\n",
    "        else: # is the actress\n",
    "            actress_id = actress_soups[i].attrs['href'].split('/')[2] #get id\n",
    "            actress_ids.append(actress_id)\n",
    "            actress_name = actress_soups[i].find('img').attrs['alt'] #get name\n",
    "            actress_names.append(actress_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "actor_points = []\n",
    "actor_names = []\n",
    "actor_ids = []\n",
    "for j in range(1, 10):\n",
    "    # initiate a new chrome session\n",
    "    executable_path = {'executable_path': ChromeDriverManager().install()}\n",
    "    browser = Browser('chrome', **executable_path, headless=False)\n",
    "    url = f\"https://www.imdb.com/list/ls022928819/?page={j}\"\n",
    "    browser.visit(url)\n",
    "    html = browser.html\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    sleep(randint(1, 3))\n",
    "    browser.quit()\n",
    "    point_soups = soup.find_all('div', class_=\"list-description\")\n",
    "    #scrap attribute =a , href contain /name/nm\n",
    "    actor_soups = soup.find_all('a', href=re.compile(\"/name/nm\"))\n",
    "    # scrape points\n",
    "    for i in range(1, len(point_soups)):\n",
    "        try:\n",
    "            point = int(point_soups[i].find('p').text.split()[0])\n",
    "            actor_points.append(point)\n",
    "        except IndexError:\n",
    "            break\n",
    "    #scrape actresses names and ids\n",
    "    for i in range(0, len(actor_soups)):\n",
    "        x = actor_soups[i].find('img')\n",
    "        #check if the attribute has image attribute\n",
    "        if x is None:  #not the actress\n",
    "            continue\n",
    "        else: # is the actress\n",
    "            id = actor_soups[i].attrs['href'].split('/')[2] #get id\n",
    "            actor_ids.append(id)\n",
    "            name = actor_soups[i].find('img').attrs['alt'] #get name\n",
    "            actor_names.append(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "popular_actress = pd.DataFrame({'ids':actress_ids,\n",
    "                                      'names':actress_names,\n",
    "                                     'points':actress_points})\n",
    "popular_actor = pd.DataFrame({'ids':actor_ids,\n",
    "                                      'names':actor_names,\n",
    "                                     'points':actor_points})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "popular_actress.to_csv('./data/popular_actress.csv',index = False)\n",
    "popular_actor.to_csv('./data/popular_actor.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "popular_actor_actress = pd.concat([popular_actor,popular_actress],ignore_index=True)\n",
    "popular_actor_actress.to_csv('./data/popular_actor_actress.csv',index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('bootcamp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a7c284f6c3454d9296f827a682b0575908dd97079acfefde70d6f996863eb3eb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
